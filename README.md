## üõ†Ô∏è How to Run the Challenges

**Step 1** Clone the repository
```
git clone https://github.com/punitdarji/AI-Security-Challenges
```
```
cd AI-Security-Challenges
```

Requirements üì¶

**Step 2** Create requirements.txt: In the same folder, create a file named requirements.txt and add the following lines to it:
```
pip install -r requirements.txt 
```
**OR** You can run the following command
```pip install pydantic==1.10.13
```

**Step 3** Run the Specific Challenge: Execute the main application file for the challenge you wish to attempt.

```
cd Prompt Injection and Data Leakage
python main.py
```

‚ñ∂Ô∏è Video Tutorials

Watch the accompanying video series to understand the theory, code, and solution for each of these LLM security flaws:

- [YouTube Link: AI Reasoning Leakage (Prompt Injection)](https://youtu.be/Q3h10iq_KLo)

- [YouTube Link: Insecure Output Handling](https://youtu.be/uVHTruIjUSI)

- [YouTube Link: Training Data Poisoning Attack](https://youtu.be/NF4S1WbatzY)

- [YouTube Link: Denial of Service (DoS) Attack in LLM](https://youtu.be/hZnqThW41RU)

- [YouTube Link: Supply Chain Attacks in LLM Apps](https://youtu.be/S32mxhRIvbk)

- [YouTube Link: Insecure Plugin Design Vulnerability AI](https://youtu.be/oPqbGSugg-U)

- [YouTube Link: Excessive Agency and Data Leakage](https://youtu.be/oU7HsnKRemc)

- [YouTube Link: AI Reasoning: Functionality or Vulnerability?](https://youtu.be/jcOPUsfUYZY)

- [YouTube Link: Overreliance and Command Injection]()

